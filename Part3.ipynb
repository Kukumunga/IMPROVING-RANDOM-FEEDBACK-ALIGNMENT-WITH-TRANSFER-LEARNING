{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YvvghZsdYb2d","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import gzip\n","import numpy\n","from six.moves import xrange  # pylint: disable=redefined-builtin\n","\n","from tensorflow.contrib.learn.python.learn.datasets import base\n","from tensorflow.python.framework import dtypes\n","\n","#loading weight file into variable\n","weights_file = np.load('weights.npz')\n","w1_file = weights_file['w1']\n","w2_file = weights_file['w2']\n","#print(w1.shape) \n","#print(w2.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1M6pbipZREh9","colab_type":"code","colab":{}},"source":["#########for emnist-letters########\n","# from https://github.com/tensorflow/tensorflow/blob/7c36309c37b04843030664cdc64aca2bb7d6ecaa/tensorflow/contrib/learn/python/learn/datasets/mnist.py#L160\n","# modified for 26 classes\n","def _read32(bytestream):\n","  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n","  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n","\n","\n","def extract_images(f):\n","  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n","  Args:\n","    f: A file object that can be passed into a gzip reader.\n","  Returns:\n","    data: A 4D uint8 numpy array [index, y, x, depth].\n","  Raises:\n","    ValueError: If the bytestream does not start with 2051.\n","  \"\"\"\n","  print('Extracting', f.name)\n","  with gzip.GzipFile(fileobj=f) as bytestream:\n","    magic = _read32(bytestream)\n","    if magic != 2051:\n","      raise ValueError('Invalid magic number %d in MNIST image file: %s' %\n","                       (magic, f.name))\n","    num_images = _read32(bytestream)\n","    rows = _read32(bytestream)\n","    cols = _read32(bytestream)\n","    buf = bytestream.read(rows * cols * num_images)\n","    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n","    data = data.reshape(num_images, rows, cols, 1)\n","    return data\n","\n","\n","def dense_to_one_hot(labels_dense, num_classes):\n","  labels_dense = labels_dense - 1 #ADDED THIS BC INDEX STARTS AT 1\n","  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n","  num_labels = labels_dense.shape[0]\n","  index_offset = numpy.arange(num_labels) * num_classes\n","  labels_one_hot = numpy.zeros((num_labels, num_classes))\n","  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n","  return labels_one_hot\n","\n","\n","def extract_labels(f, one_hot=False, num_classes=26):\n","  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\n","  Args:\n","    f: A file object that can be passed into a gzip reader.\n","    one_hot: Does one hot encoding for the result.\n","    num_classes: Number of classes for the one hot encoding.\n","  Returns:\n","    labels: a 1D uint8 numpy array.\n","  Raises:\n","    ValueError: If the bystream doesn't start with 2049.\n","  \"\"\"\n","  print('Extracting', f.name)\n","  with gzip.GzipFile(fileobj=f) as bytestream:\n","    magic = _read32(bytestream)\n","    if magic != 2049:\n","      raise ValueError('Invalid magic number %d in MNIST label file: %s' %\n","                       (magic, f.name))\n","    num_items = _read32(bytestream)\n","    buf = bytestream.read(num_items)\n","    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n","    if one_hot:\n","      return dense_to_one_hot(labels, num_classes)\n","    return labels\n","\n","\n","class DataSet(object):\n","\n","  def __init__(self,\n","               images,\n","               labels,\n","               fake_data=False,\n","               one_hot=False,\n","               dtype=dtypes.float32,\n","               reshape=True):\n","    \"\"\"Construct a DataSet.\n","    one_hot arg is used only if fake_data is true.  `dtype` can be either\n","    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n","    `[0, 1]`.\n","    \"\"\"\n","    dtype = dtypes.as_dtype(dtype).base_dtype\n","    if dtype not in (dtypes.uint8, dtypes.float32):\n","      raise TypeError('Invalid image dtype %r, expected uint8 or float32' %\n","                      dtype)\n","    if fake_data:\n","      self._num_examples = 10000\n","      self.one_hot = one_hot\n","    else:\n","      assert images.shape[0] == labels.shape[0], (\n","          'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n","      self._num_examples = images.shape[0]\n","\n","      # Convert shape from [num examples, rows, columns, depth]\n","      # to [num examples, rows*columns] (assuming depth == 1)\n","      if reshape:\n","        assert images.shape[3] == 1\n","        images = images.reshape(images.shape[0],\n","                                images.shape[1] * images.shape[2])\n","      if dtype == dtypes.float32:\n","        # Convert from [0, 255] -> [0.0, 1.0].\n","        images = images.astype(numpy.float32)\n","        images = numpy.multiply(images, 1.0 / 255.0)\n","    self._images = images\n","    self._labels = labels\n","    self._epochs_completed = 0\n","    self._index_in_epoch = 0\n","\n","  @property\n","  def images(self):\n","    return self._images\n","\n","  @property\n","  def labels(self):\n","    return self._labels\n","\n","  @property\n","  def num_examples(self):\n","    return self._num_examples\n","\n","  @property\n","  def epochs_completed(self):\n","    return self._epochs_completed\n","\n","  def next_batch(self, batch_size, fake_data=False):\n","    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n","    if fake_data:\n","      fake_image = [1] * 784\n","      if self.one_hot:\n","        fake_label = [1] + [0] * 9\n","      else:\n","        fake_label = 0\n","      return [fake_image for _ in xrange(batch_size)], [\n","          fake_label for _ in xrange(batch_size)\n","      ]\n","    start = self._index_in_epoch\n","    self._index_in_epoch += batch_size\n","    if self._index_in_epoch > self._num_examples:\n","      # Finished epoch\n","      self._epochs_completed += 1\n","      # Shuffle the data\n","      perm = numpy.arange(self._num_examples)\n","      numpy.random.shuffle(perm)\n","      self._images = self._images[perm]\n","      self._labels = self._labels[perm]\n","      # Start next epoch\n","      start = 0\n","      self._index_in_epoch = batch_size\n","      assert batch_size <= self._num_examples\n","    end = self._index_in_epoch\n","    return self._images[start:end], self._labels[start:end]\n","\n","\n","def read_data_sets(train_dir,\n","                   fake_data=False,\n","                   one_hot=False,\n","                   dtype=dtypes.float32,\n","                   reshape=True,\n","                   validation_size=5000):\n","  if fake_data:\n","\n","    def fake():\n","      return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)\n","\n","    train = fake()\n","    validation = fake()\n","    test = fake()\n","    return base.Datasets(train=train, validation=validation, test=test)\n","\n","  TRAIN_IMAGES = 'emnist-letters-train-images-idx3-ubyte.gz'\n","  TRAIN_LABELS = 'emnist-letters-train-labels-idx1-ubyte.gz'\n","  TEST_IMAGES = 'emnist-letters-test-images-idx3-ubyte.gz'\n","  TEST_LABELS = 'emnist-letters-test-labels-idx1-ubyte.gz'\n","\n","  local_file = TRAIN_IMAGES #base.maybe_download(TRAIN_IMAGES, train_dir,\n","                                   #SOURCE_URL + TRAIN_IMAGES)\n","  with open(local_file, 'rb') as f:\n","    train_images = extract_images(f)\n","\n","  local_file = TRAIN_LABELS #base.maybe_download(TRAIN_LABELS, train_dir,\n","                                   #SOURCE_URL + TRAIN_LABELS)\n","  with open(local_file, 'rb') as f:\n","    train_labels = extract_labels(f, one_hot=one_hot)\n","\n","  local_file = TEST_IMAGES #base.maybe_download(TEST_IMAGES, train_dir,\n","                                  # SOURCE_URL + TEST_IMAGES)\n","  with open(local_file, 'rb') as f:\n","    test_images = extract_images(f)\n","\n","  local_file = TEST_LABELS #base.maybe_download(TEST_LABELS, train_dir,\n","                                   #SOURCE_URL + TEST_LABELS)\n","  with open(local_file, 'rb') as f:\n","    test_labels = extract_labels(f, one_hot=one_hot)\n","\n","  if not 0 <= validation_size <= len(train_images):\n","    raise ValueError(\n","        'Validation size should be between 0 and {}. Received: {}.'\n","        .format(len(train_images), validation_size))\n","\n","  validation_images = train_images[:validation_size]\n","  validation_labels = train_labels[:validation_size]\n","  train_images = train_images[validation_size:]\n","  train_labels = train_labels[validation_size:]\n","\n","  train = DataSet(train_images, train_labels, dtype=dtype, reshape=reshape)\n","  validation = DataSet(validation_images,\n","                       validation_labels,\n","                       dtype=dtype,\n","                       reshape=reshape)\n","  test = DataSet(test_images, test_labels, dtype=dtype, reshape=reshape)\n","\n","  return base.Datasets(train=train, validation=validation, test=test)\n","\n","\n","def load_mnist(train_dir='MNIST-data'):\n","  return read_data_sets(train_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-xfyLpCI1Px","colab_type":"code","outputId":"8477238f-7494-46cb-b6b7-48e960f7894e","executionInfo":{"status":"ok","timestamp":1555723074278,"user_tz":240,"elapsed":1266,"user":{"displayName":"gabi kim","photoUrl":"https://lh5.googleusercontent.com/-EmHANwJCol0/AAAAAAAAAAI/AAAAAAAAAGE/OTznggbdZi8/s64/photo.jpg","userId":"06422217761192518495"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["#Importing emnist dataset from files\n","#NOTE: load dataset (gzip one) from https://www.nist.gov/node/1298471/emnist-dataset\n","emnist = read_data_sets(\"/\", one_hot=True) \n","print(len(emnist.test.labels))\n","print(len(emnist.train.images))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Extracting emnist-letters-train-images-idx3-ubyte.gz\n","Extracting emnist-letters-train-labels-idx1-ubyte.gz\n","Extracting emnist-letters-test-images-idx3-ubyte.gz\n","Extracting emnist-letters-test-labels-idx1-ubyte.gz\n","20800\n","119800\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P26Nq_qxVv1P","colab_type":"code","colab":{}},"source":["#########Andrej's method taken from: http://blog.aloni.org/posts/backprop-with-tensorflow/ ##########\n","middle = 30\n","num_out = 26\n","\n","a_0 = tf.placeholder(tf.float32, [None, 784])\n","y = tf.placeholder(tf.float32, [None, num_out])\n","\n","### Comment out whichever u are not using\n","w_1 = tf.Variable(tf.truncated_normal([784, middle])) #tf.Variable(w1_file) \n","b_1 = tf.Variable(tf.truncated_normal([1, middle]))\n","w_2 = tf.Variable(tf.truncated_normal([middle, num_out]))\n","b_2 = tf.Variable(tf.truncated_normal([1, num_out]))\n","\n","\n","are = w_2\n","def sigma(x):\n","    return tf.div(tf.constant(1.0),\n","                  tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n","  \n","  \n","z_1 = tf.add(tf.matmul(a_0, w_1), b_1)\n","a_1 = sigma(z_1)\n","z_2 = tf.add(tf.matmul(a_1, w_2), b_2)\n","a_2 = sigma(z_2)\n","\n","\n","diff = tf.subtract(a_2, y)\n","\n","\n","def sigmaprime(x):\n","    return tf.multiply(sigma(x), tf.subtract(tf.constant(1.0), sigma(x)))\n","  \n","  \n","d_z_2 = tf.multiply(diff, sigmaprime(z_2))\n","d_b_2 = d_z_2\n","d_w_2 = tf.matmul(tf.transpose(a_1), d_z_2)\n","\n","d_a_1 = tf.matmul(d_z_2, tf.transpose(w_2))\n","d_z_1 = tf.multiply(d_a_1, sigmaprime(z_1))\n","d_b_1 = d_z_1\n","d_w_1 = tf.matmul(tf.transpose(a_0), d_z_1)\n","\n","\n","eta = tf.constant(0.5)\n","step = [\n","    tf.assign(w_1,\n","            tf.subtract(w_1, tf.multiply(eta, d_w_1)))\n","  , tf.assign(b_1,\n","            tf.subtract(b_1, tf.multiply(eta,\n","                               tf.reduce_mean(d_b_1, axis=[0]))))\n","  , tf.assign(w_2,\n","            tf.subtract(w_2, tf.multiply(eta, d_w_2)))\n","  , tf.assign(b_2,\n","            tf.subtract(b_2, tf.multiply(eta,\n","                               tf.reduce_mean(d_b_2, axis=[0]))))\n","]\n","\n","\n","# loss functions \n","acct_mat = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1))\n","acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n","\n","\n","#initialize some variables\n","batch_size = 10\n","total_test_size = len(emnist.test.images)\n","epoch = range(300)\n","test_accuracy = []\n","train_accuracy = []\n","\n","#start session\n","sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())\n","\n","### train the model\n","for j in epoch:\n","\n","  for i in range(total_test_size//batch_size):\n","      batch_xs, batch_ys = emnist.train.next_batch(batch_size)\n","      sess.run(step, feed_dict = {a_0: batch_xs,\n","                                  y : batch_ys})\n","  ### test the accuracy \n","  test_acc = sess.run(acct_res, feed_dict =\n","                          {a_0: emnist.test.images[:1000],\n","                           y : emnist.test.labels[:1000]})/1000\n","  train_acc = sess.run(acct_res, feed_dict =\n","                          {a_0: emnist.train.images[:1000],\n","                           y : emnist.train.labels[:1000]})/1000\n","  test_accuracy.append(test_acc)\n","  train_accuracy.append(train_acc)\n","  print(j)\n","  \n","#close the session  \n","sess.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0IJfxYDbjZ2","colab_type":"code","colab":{}},"source":["#plot \n","plt.figure(figsize=(12, 6))\n","plt.title('EMNIST-letters using inital weights from normal distribution')\n","#plt.title('EMNIST-letters using initial first layer weights from MNIST-digits')\n","plt.plot(epoch,test_accuracy, label='test')\n","plt.plot(epoch,train_accuracy, label='train')\n","plt.xlabel('epoch')\n","plt.ylabel('accuracy')\n","plt.legend()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q8NK4b0tlFWn","colab_type":"code","outputId":"a1d8c372-5dbe-489d-a83f-a3e227dc7202","executionInfo":{"status":"ok","timestamp":1554251014626,"user_tz":240,"elapsed":1223,"user":{"displayName":"gabi kim","photoUrl":"https://lh5.googleusercontent.com/-EmHANwJCol0/AAAAAAAAAAI/AAAAAAAAAGE/OTznggbdZi8/s64/photo.jpg","userId":"06422217761192518495"}},"colab":{"base_uri":"https://localhost:8080/","height":459}},"source":["print(emnist.train.next_batch(10))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YcM-SjbLqiVH","colab_type":"code","colab":{}},"source":["####get train_accuracy_w1 and normal from text file\n","#plot \n","plt.figure(figsize=(12, 6))\n","plt.title('EMNIST-letters test data using transferred initial weights vs. normal distribution (zoomed)')\n","plt.plot(epoch,test_accuracy_normal, label='normal dist')\n","plt.plot(epoch,test_accuracy_usingw1, label='transferred weights')\n","plt.xlabel('epoch')\n","plt.ylabel('accuracy')\n","plt.legend()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v368kTFjqIXm","colab_type":"text"},"source":[""]}]}