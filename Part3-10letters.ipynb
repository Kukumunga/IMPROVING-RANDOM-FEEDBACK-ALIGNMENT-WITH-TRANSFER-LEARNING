{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part3-10letters.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"YvvghZsdYb2d","colab_type":"code","outputId":"01d4b95c-ad36-45b2-feb0-998f93aa5361","executionInfo":{"status":"ok","timestamp":1555722514942,"user_tz":240,"elapsed":3365,"user":{"displayName":"gabi kim","photoUrl":"https://lh5.googleusercontent.com/-EmHANwJCol0/AAAAAAAAAAI/AAAAAAAAAGE/OTznggbdZi8/s64/photo.jpg","userId":"06422217761192518495"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import gzip\n","import numpy\n","from six.moves import xrange  # pylint: disable=redefined-builtin\n","\n","from tensorflow.contrib.learn.python.learn.datasets import base\n","from tensorflow.python.framework import dtypes\n","\n","from sklearn.decomposition import PCA\n","\n","#loading weight file into variable\n","weights_file = np.load('weights.npz')\n","w1_file = weights_file['w1']\n","w2_file = weights_file['w2']\n","print(w1_file.shape) \n","\n","print(w2_file.shape)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["(784, 30)\n","(30, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"1M6pbipZREh9","colab_type":"code","colab":{}},"cell_type":"code","source":["#########for emnist-letters########\n","# from https://github.com/tensorflow/tensorflow/blob/7c36309c37b04843030664cdc64aca2bb7d6ecaa/tensorflow/contrib/learn/python/learn/datasets/mnist.py#L160\n","# modified for 10 classes of LETTERS\n","def _read32(bytestream):\n","  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n","  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n","\n","\n","def extract_images(f):\n","  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n","  Args:\n","    f: A file object that can be passed into a gzip reader.\n","  Returns:\n","    data: A 4D uint8 numpy array [index, y, x, depth].\n","  Raises:\n","    ValueError: If the bytestream does not start with 2051.\n","  \"\"\"\n","  print('Extracting', f.name)\n","  with gzip.GzipFile(fileobj=f) as bytestream:\n","    magic = _read32(bytestream)\n","    if magic != 2051:\n","      raise ValueError('Invalid magic number %d in MNIST image file: %s' %\n","                       (magic, f.name))\n","    num_images = _read32(bytestream)\n","    rows = _read32(bytestream)\n","    cols = _read32(bytestream)\n","    buf = bytestream.read(rows * cols * num_images)\n","    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n","    data = data.reshape(num_images, rows, cols, 1)\n","    return data\n","\n","\n","def dense_to_one_hot(labels_dense, num_classes):\n","  labels_dense = labels_dense - 1 #ADDED THIS BC INDEX STARTS AT 1\n","  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n","  num_labels = labels_dense.shape[0]\n","  index_offset = numpy.arange(num_labels) * num_classes\n","  labels_one_hot = numpy.zeros((num_labels, num_classes))\n","  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n","  return labels_one_hot\n","\n","\n","def extract_labels(f, one_hot=False, num_classes=26):\n","  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\n","  Args:\n","    f: A file object that can be passed into a gzip reader.\n","    one_hot: Does one hot encoding for the result.\n","    num_classes: Number of classes for the one hot encoding.\n","  Returns:\n","    labels: a 1D uint8 numpy array.\n","  Raises:\n","    ValueError: If the bystream doesn't start with 2049.\n","  \"\"\"\n","  print('Extracting', f.name)\n","  with gzip.GzipFile(fileobj=f) as bytestream:\n","    magic = _read32(bytestream)\n","    if magic != 2049:\n","      raise ValueError('Invalid magic number %d in MNIST label file: %s' %\n","                       (magic, f.name))\n","    num_items = _read32(bytestream)\n","    buf = bytestream.read(num_items)\n","    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n","    if one_hot:\n","      return dense_to_one_hot(labels, num_classes)\n","    return labels\n","\n","\n","class DataSet(object):\n","\n","  def __init__(self,\n","               images,\n","               labels,\n","               fake_data=False,\n","               one_hot=False,\n","               dtype=dtypes.float32,\n","               reshape=True):\n","    \"\"\"Construct a DataSet.\n","    one_hot arg is used only if fake_data is true.  `dtype` can be either\n","    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n","    `[0, 1]`.\n","    \"\"\"\n","    dtype = dtypes.as_dtype(dtype).base_dtype\n","    if dtype not in (dtypes.uint8, dtypes.float32):\n","      raise TypeError('Invalid image dtype %r, expected uint8 or float32' %\n","                      dtype)\n","    if fake_data:\n","      self._num_examples = 10000\n","      self.one_hot = one_hot\n","    else:\n","      assert images.shape[0] == labels.shape[0], (\n","          'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n","      self._num_examples = images.shape[0]\n","\n","      # Convert shape from [num examples, rows, columns, depth]\n","      # to [num examples, rows*columns] (assuming depth == 1)\n","      if reshape:\n","        assert images.shape[3] == 1\n","        images = images.reshape(images.shape[0],\n","                                images.shape[1] * images.shape[2])\n","      if dtype == dtypes.float32:\n","        # Convert from [0, 255] -> [0.0, 1.0].\n","        images = images.astype(numpy.float32)\n","        images = numpy.multiply(images, 1.0 / 255.0)\n","    self._images = images\n","    self._labels = labels\n","    self._epochs_completed = 0\n","    self._index_in_epoch = 0\n","\n","  @property\n","  def images(self):\n","    return self._images\n","\n","  @property\n","  def labels(self):\n","    return self._labels\n","\n","  @property\n","  def num_examples(self):\n","    return self._num_examples\n","\n","  @property\n","  def epochs_completed(self):\n","    return self._epochs_completed\n","\n","  def next_batch(self, batch_size, fake_data=False):\n","    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n","    if fake_data:\n","      fake_image = [1] * 784\n","      if self.one_hot:\n","        fake_label = [1] + [0] * 9\n","      else:\n","        fake_label = 0\n","      return [fake_image for _ in xrange(batch_size)], [\n","          fake_label for _ in xrange(batch_size)\n","      ]\n","    start = self._index_in_epoch\n","    self._index_in_epoch += batch_size\n","    if self._index_in_epoch > self._num_examples:\n","      # Finished epoch\n","      self._epochs_completed += 1\n","      # Shuffle the data\n","      perm = numpy.arange(self._num_examples)\n","      numpy.random.shuffle(perm)\n","      self._images = self._images[perm]\n","      self._labels = self._labels[perm]\n","      # Start next epoch\n","      start = 0\n","      self._index_in_epoch = batch_size\n","      assert batch_size <= self._num_examples\n","    end = self._index_in_epoch\n","    return self._images[start:end], self._labels[start:end]\n","\n","def remove_data(labels, images):\n","  labels_10 = []\n","  images_10 = []\n","  \n","  for i in range(len(labels)): #rows\n","    for j in range(0,10): #columns\n","      if labels[i][j] == 1:\n","        labels_10.append(labels[i][0:10])\n","        images_10.append(images[i])\n","  \n","  labels_10 = np.asarray(labels_10)\n","  images_10 = np.asarray(images_10)\n","\n","  return labels_10, images_10\n","  \n","def read_data_sets(train_dir,\n","                   fake_data=False,\n","                   one_hot=False,\n","                   dtype=dtypes.float32,\n","                   reshape=True,\n","                   validation_size=5000):\n","  if fake_data:\n","\n","    def fake():\n","      return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)\n","\n","    train = fake()\n","    validation = fake()\n","    test = fake()\n","    return base.Datasets(train=train, validation=validation, test=test)\n","\n","  TRAIN_IMAGES = 'emnist-letters-train-images-idx3-ubyte.gz'\n","  TRAIN_LABELS = 'emnist-letters-train-labels-idx1-ubyte.gz'\n","  TEST_IMAGES = 'emnist-letters-test-images-idx3-ubyte.gz'\n","  TEST_LABELS = 'emnist-letters-test-labels-idx1-ubyte.gz'\n","\n","  local_file = TRAIN_IMAGES #base.maybe_download(TRAIN_IMAGES, train_dir,\n","                                   #SOURCE_URL + TRAIN_IMAGES)\n","  with open(local_file, 'rb') as f:\n","    train_images = extract_images(f)\n","\n","  local_file = TRAIN_LABELS #base.maybe_download(TRAIN_LABELS, train_dir,\n","                                   #SOURCE_URL + TRAIN_LABELS)\n","  with open(local_file, 'rb') as f:\n","    train_labels = extract_labels(f, one_hot=one_hot)\n","\n","  train_labels, train_images = remove_data(train_labels, train_images)\n","  \n","  local_file = TEST_IMAGES #base.maybe_download(TEST_IMAGES, train_dir,\n","                                  # SOURCE_URL + TEST_IMAGES)\n","  with open(local_file, 'rb') as f:\n","    test_images = extract_images(f)\n","\n","  local_file = TEST_LABELS #base.maybe_download(TEST_LABELS, train_dir,\n","                                   #SOURCE_URL + TEST_LABELS)\n","  with open(local_file, 'rb') as f:\n","    test_labels = extract_labels(f, one_hot=one_hot)\n","\n","  test_labels, test_images = remove_data(test_labels, test_images)\n","  \n","  if not 0 <= validation_size <= len(train_images):\n","    raise ValueError(\n","        'Validation size should be between 0 and {}. Received: {}.'\n","        .format(len(train_images), validation_size))\n","\n","  validation_images = train_images[:validation_size]\n","  validation_labels = train_labels[:validation_size]\n","  train_images = train_images[validation_size:]\n","  train_labels = train_labels[validation_size:]\n","\n","  train = DataSet(train_images, train_labels, dtype=dtype, reshape=reshape)\n","  validation = DataSet(validation_images,\n","                       validation_labels,\n","                       dtype=dtype,\n","                       reshape=reshape)\n","  test = DataSet(test_images, test_labels, dtype=dtype, reshape=reshape)\n","\n","  return base.Datasets(train=train, validation=validation, test=test)\n","\n","\n","def load_mnist(train_dir='MNIST-data'):\n","  return read_data_sets(train_dir)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X-xfyLpCI1Px","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"48e8638f-d410-4209-b71b-65aec9326a32","executionInfo":{"status":"ok","timestamp":1555723081704,"user_tz":240,"elapsed":1773,"user":{"displayName":"gabi kim","photoUrl":"https://lh5.googleusercontent.com/-EmHANwJCol0/AAAAAAAAAAI/AAAAAAAAAGE/OTznggbdZi8/s64/photo.jpg","userId":"06422217761192518495"}}},"cell_type":"code","source":["#Importing emnist dataset from files\n","#NOTE: load dataset (gzip one) from https://www.nist.gov/node/1298471/emnist-dataset\n","emnist = read_data_sets(\"/\", one_hot=True) \n","print(len(emnist.test.images))\n","print(len(emnist.train.images))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Extracting emnist-letters-train-images-idx3-ubyte.gz\n","Extracting emnist-letters-train-labels-idx1-ubyte.gz\n","Extracting emnist-letters-test-images-idx3-ubyte.gz\n","Extracting emnist-letters-test-labels-idx1-ubyte.gz\n","8000\n","43000\n"],"name":"stdout"}]},{"metadata":{"id":"P26Nq_qxVv1P","colab_type":"code","colab":{}},"cell_type":"code","source":["#########Andrej's method taken from: http://blog.aloni.org/posts/backprop-with-tensorflow/ ##########\n","middle = 30\n","num_out = 10\n","\n","a_0 = tf.placeholder(tf.float32, [None, 784])\n","y = tf.placeholder(tf.float32, [None, num_out])\n","\n","### Comment out whichever u are not using\n","w_1 = tf.Variable(tf.truncated_normal([784, middle])) #tf.Variable(w1_file) \n","b_1 = tf.Variable(tf.truncated_normal([1, middle]))\n","w_2 = tf.Variable(w2_file) #tf.Variable(tf.truncated_normal([middle, num_out])) #tf.Variable(w2_file)\n","b_2 = tf.Variable(tf.truncated_normal([1, num_out]))\n","\n","\n","are = w_2\n","def sigma(x):\n","    return tf.div(tf.constant(1.0),\n","                  tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n","  \n","  \n","z_1 = tf.add(tf.matmul(a_0, w_1), b_1)\n","a_1 = sigma(z_1)\n","z_2 = tf.add(tf.matmul(a_1, w_2), b_2)\n","a_2 = sigma(z_2)\n","\n","\n","diff = tf.subtract(a_2, y)\n","\n","\n","def sigmaprime(x):\n","    return tf.multiply(sigma(x), tf.subtract(tf.constant(1.0), sigma(x)))\n","  \n","  \n","d_z_2 = tf.multiply(diff, sigmaprime(z_2))\n","d_b_2 = d_z_2\n","d_w_2 = tf.matmul(tf.transpose(a_1), d_z_2)\n","\n","d_a_1 = tf.matmul(d_z_2, tf.transpose(w_2))\n","d_z_1 = tf.multiply(d_a_1, sigmaprime(z_1))\n","d_b_1 = d_z_1\n","d_w_1 = tf.matmul(tf.transpose(a_0), d_z_1)\n","\n","\n","eta = tf.constant(0.5)\n","step = [\n","    tf.assign(w_1,\n","            tf.subtract(w_1, tf.multiply(eta, d_w_1)))\n","  , tf.assign(b_1,\n","            tf.subtract(b_1, tf.multiply(eta,\n","                               tf.reduce_mean(d_b_1, axis=[0]))))\n","  , tf.assign(w_2,\n","            tf.subtract(w_2, tf.multiply(eta, d_w_2)))\n","  , tf.assign(b_2,\n","            tf.subtract(b_2, tf.multiply(eta,\n","                               tf.reduce_mean(d_b_2, axis=[0]))))\n","]\n","\n","\n","# loss functions \n","acct_mat = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1))\n","acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n","\n","\n","#initialize some variables\n","batch_size = 10\n","total_test_size = len(emnist.test.images)\n","epoch = range(300)\n","test_accuracy = []\n","train_accuracy = []\n","\n","#start session\n","sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())\n","\n","### train the model\n","for j in epoch:\n","\n","  for i in range(total_test_size//batch_size):\n","      batch_xs, batch_ys = emnist.train.next_batch(batch_size)\n","      sess.run(step, feed_dict = {a_0: batch_xs,\n","                                  y : batch_ys})\n","  ### test the accuracy \n","  test_acc = sess.run(acct_res, feed_dict =\n","                          {a_0: emnist.test.images[:1000],\n","                           y : emnist.test.labels[:1000]})/1000\n","  train_acc = sess.run(acct_res, feed_dict =\n","                          {a_0: emnist.train.images[:1000],\n","                           y : emnist.train.labels[:1000]})/1000\n","  test_accuracy.append(test_acc)\n","  train_accuracy.append(train_acc)\n","  print(\n","            \"Iteration\",\n","            str(j),\n","            \"\\t| test accuracy =\",\n","            str(test_acc),\n","            \"\\t| train accuracy =\",\n","            str(train_acc)\n","            )\n","  \n","#close the session  \n","sess.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r0IJfxYDbjZ2","colab_type":"code","colab":{}},"cell_type":"code","source":["#plot \n","plt.figure(figsize=(12, 6))\n","#plt.title('EMNIST-letters using inital weights from normal distribution')\n","plt.title('10 EMNIST-letters using second layer of weights from MNIST-digits')\n","plt.plot(epoch, test_accuracy, label='test')\n","plt.plot(epoch, train_accuracy, label='train')\n","plt.xlabel('epoch')\n","plt.ylabel('accuracy')\n","plt.legend()\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bUA9OWEzp2x8","colab_type":"code","colab":{}},"cell_type":"code","source":["print(train_accuracy) \n","print(test_accuracy)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v57Uv7K1qDHv","colab_type":"code","colab":{}},"cell_type":"code","source":["test_accuracy_w1w2 = [0.042, 0.104, 0.159, 0.152, 0.146, 0.153, 0.149, 0.163, 0.159, 0.163, 0.162, 0.173, 0.145, 0.166, 0.164, 0.152, 0.172, 0.145, 0.181, 0.173, 0.185, 0.227, 0.253, 0.814, 0.699, 0.715, 0.812, 0.814, 0.764, 0.797, 0.816, 0.74, 0.82, 0.824, 0.769, 0.855, 0.833, 0.749, 0.834, 0.808, 0.835, 0.889, 0.831, 0.818, 0.817, 0.868, 0.823, 0.88, 0.844, 0.841, 0.849, 0.864, 0.892, 0.872, 0.88, 0.786, 0.889, 0.834, 0.818, 0.849, 0.858, 0.866, 0.834, 0.836, 0.833, 0.836, 0.859, 0.89, 0.833, 0.85, 0.835, 0.835, 0.836, 0.87, 0.845, 0.852, 0.895, 0.808, 0.835, 0.854, 0.803, 0.857, 0.878, 0.782, 0.865, 0.856, 0.844, 0.869, 0.844, 0.806, 0.822, 0.871, 0.778, 0.867, 0.869, 0.887, 0.845, 0.916, 0.875, 0.89, 0.844, 0.821, 0.831, 0.825, 0.881, 0.857, 0.872, 0.853, 0.831, 0.838, 0.861, 0.851, 0.849, 0.838, 0.855, 0.845, 0.83, 0.81, 0.82, 0.871, 0.865, 0.848, 0.814, 0.829, 0.83, 0.85, 0.83, 0.847, 0.823, 0.877, 0.872, 0.863, 0.816, 0.847, 0.791, 0.887, 0.851, 0.865, 0.858, 0.873, 0.831, 0.861, 0.852, 0.873, 0.872, 0.865, 0.881, 0.816, 0.848, 0.809, 0.885, 0.843, 0.88, 0.824, 0.838, 0.881, 0.859, 0.82, 0.873, 0.845, 0.87, 0.859, 0.886, 0.878, 0.844, 0.892, 0.857, 0.828, 0.838, 0.889, 0.852, 0.872, 0.846, 0.875, 0.845, 0.858, 0.862, 0.875, 0.86, 0.865, 0.877, 0.866, 0.873, 0.842, 0.86, 0.846, 0.868, 0.893, 0.855, 0.838, 0.813, 0.852, 0.846, 0.907, 0.84, 0.827, 0.828, 0.885, 0.896, 0.864, 0.844, 0.829, 0.853, 0.85, 0.866, 0.857, 0.856, 0.85, 0.863, 0.822, 0.861, 0.87, 0.873, 0.875, 0.849, 0.856, 0.825, 0.868, 0.863, 0.868, 0.864, 0.84, 0.875, 0.869, 0.837, 0.845, 0.875, 0.856, 0.841, 0.852, 0.836, 0.852, 0.853, 0.868, 0.831, 0.83, 0.836, 0.867, 0.88, 0.877, 0.874, 0.872, 0.86, 0.839, 0.871, 0.822, 0.852, 0.816, 0.869, 0.895, 0.865, 0.867, 0.852, 0.854, 0.814, 0.871, 0.826, 0.862, 0.87, 0.862, 0.847, 0.882, 0.858, 0.855, 0.873, 0.842, 0.842, 0.872, 0.832, 0.864, 0.863, 0.875, 0.901, 0.833, 0.849, 0.867, 0.862, 0.867, 0.854, 0.885, 0.834, 0.861, 0.852, 0.867, 0.858, 0.844, 0.841, 0.863, 0.874, 0.868, 0.849, 0.866, 0.871, 0.856, 0.864, 0.84, 0.864, 0.861, 0.854, 0.854]\n","train_accuracy_w1w2 = [0.187, 0.251, 0.319, 0.341, 0.378, 0.422, 0.426, 0.416, 0.422, 0.414, 0.437, 0.435, 0.435, 0.469, 0.512, 0.513, 0.508, 0.506, 0.516, 0.52, 0.52, 0.525, 0.541, 0.585, 0.574, 0.591, 0.618, 0.608, 0.6, 0.601, 0.604, 0.597, 0.61, 0.6, 0.596, 0.599, 0.603, 0.617, 0.614, 0.607, 0.614, 0.605, 0.606, 0.608, 0.612, 0.613, 0.606, 0.606, 0.624, 0.623, 0.628, 0.629, 0.614, 0.627, 0.617, 0.615, 0.625, 0.621, 0.611, 0.605, 0.609, 0.607, 0.6, 0.6, 0.619, 0.618, 0.619, 0.6, 0.607, 0.654, 0.628, 0.651, 0.648, 0.643, 0.638, 0.627, 0.619, 0.623, 0.622, 0.626, 0.635, 0.623, 0.628, 0.622, 0.624, 0.615, 0.628, 0.634, 0.632, 0.633, 0.629, 0.654, 0.642, 0.65, 0.637, 0.649, 0.636, 0.627, 0.627, 0.625, 0.618, 0.634, 0.622, 0.629, 0.637, 0.64, 0.684, 0.683, 0.701, 0.701, 0.706, 0.695, 0.713, 0.702, 0.694, 0.701, 0.706, 0.675, 0.723, 0.722, 0.729, 0.722, 0.724, 0.714, 0.723, 0.721, 0.725, 0.724, 0.718, 0.744, 0.736, 0.741, 0.733, 0.738, 0.699, 0.705, 0.686, 0.7, 0.701, 0.722, 0.716, 0.718, 0.71, 0.7, 0.707, 0.7, 0.702, 0.691, 0.695, 0.696, 0.751, 0.755, 0.754, 0.747, 0.737, 0.737, 0.725, 0.726, 0.727, 0.731, 0.723, 0.734, 0.729, 0.736, 0.725, 0.73, 0.723, 0.718, 0.722, 0.713, 0.708, 0.711, 0.723, 0.74, 0.726, 0.739, 0.732, 0.733, 0.72, 0.737, 0.73, 0.731, 0.729, 0.728, 0.716, 0.728, 0.722, 0.731, 0.733, 0.744, 0.737, 0.742, 0.737, 0.712, 0.717, 0.714, 0.721, 0.725, 0.727, 0.719, 0.723, 0.722, 0.708, 0.713, 0.73, 0.73, 0.721, 0.724, 0.726, 0.718, 0.713, 0.713, 0.699, 0.707, 0.705, 0.726, 0.725, 0.729, 0.727, 0.716, 0.708, 0.696, 0.705, 0.695, 0.694, 0.733, 0.732, 0.734, 0.721, 0.73, 0.726, 0.748, 0.748, 0.739, 0.746, 0.738, 0.734, 0.728, 0.731, 0.723, 0.722, 0.729, 0.722, 0.717, 0.717, 0.703, 0.719, 0.725, 0.731, 0.731, 0.722, 0.719, 0.743, 0.736, 0.733, 0.746, 0.734, 0.738, 0.733, 0.73, 0.729, 0.725, 0.732, 0.714, 0.712, 0.718, 0.721, 0.716, 0.705, 0.715, 0.718, 0.714, 0.711, 0.712, 0.772, 0.769, 0.768, 0.768, 0.769, 0.74, 0.745, 0.739, 0.728, 0.733, 0.753, 0.746, 0.743, 0.744, 0.748, 0.741, 0.728, 0.734, 0.733, 0.728, 0.72, 0.735, 0.741, 0.738, 0.736, 0.742]\n","train_accuracy_w1=[0.643, 0.682, 0.733, 0.766, 0.774, 0.805, 0.819, 0.84, 0.818, 0.841, 0.826, 0.828, 0.815, 0.821, 0.81, 0.822, 0.876, 0.862, 0.86, 0.868, 0.874, 0.86, 0.864, 0.871, 0.857, 0.861, 0.896, 0.88, 0.881, 0.878, 0.871, 0.864, 0.907, 0.9, 0.888, 0.893, 0.879, 0.9, 0.885, 0.877, 0.873, 0.873, 0.868, 0.871, 0.862, 0.879, 0.865, 0.862, 0.888, 0.876, 0.871, 0.867, 0.881, 0.894, 0.891, 0.882, 0.891, 0.894, 0.888, 0.892, 0.888, 0.88, 0.884, 0.887, 0.892, 0.882, 0.882, 0.882, 0.887, 0.917, 0.91, 0.92, 0.914, 0.902, 0.891, 0.897, 0.899, 0.897, 0.891, 0.883, 0.907, 0.899, 0.897, 0.889, 0.893, 0.88, 0.901, 0.886, 0.898, 0.89, 0.893, 0.914, 0.897, 0.905, 0.879, 0.893, 0.898, 0.894, 0.889, 0.89, 0.893, 0.887, 0.907, 0.877, 0.895, 0.889, 0.88, 0.914, 0.914, 0.909, 0.901, 0.895, 0.913, 0.9, 0.89, 0.895, 0.895, 0.891, 0.909, 0.913, 0.906, 0.907, 0.905, 0.903, 0.892, 0.898, 0.889, 0.897, 0.896, 0.897, 0.899, 0.899, 0.9, 0.897, 0.894, 0.897, 0.899, 0.902, 0.893, 0.92, 0.908, 0.903, 0.909, 0.909, 0.904, 0.915, 0.908, 0.911, 0.912, 0.915, 0.93, 0.922, 0.92, 0.908, 0.912, 0.924, 0.91, 0.912, 0.914, 0.911, 0.907, 0.902, 0.886, 0.903, 0.906, 0.906, 0.916, 0.916, 0.919, 0.92, 0.92, 0.913, 0.913, 0.903, 0.898, 0.911, 0.904, 0.931, 0.919, 0.917, 0.916, 0.933, 0.91, 0.918, 0.902, 0.91, 0.907, 0.907, 0.911, 0.91, 0.912, 0.906, 0.907, 0.922, 0.918, 0.913, 0.923, 0.921, 0.927, 0.918, 0.915, 0.911, 0.904, 0.915, 0.934, 0.92, 0.928, 0.924, 0.918, 0.929, 0.934, 0.907, 0.912, 0.925, 0.914, 0.905, 0.914, 0.909, 0.916, 0.909, 0.916, 0.914, 0.906, 0.91, 0.923, 0.907, 0.911, 0.909, 0.907, 0.912, 0.914, 0.91, 0.903, 0.899, 0.901, 0.904, 0.914, 0.913, 0.919, 0.908, 0.912, 0.923, 0.912, 0.926, 0.929, 0.92, 0.915, 0.905, 0.909, 0.905, 0.896, 0.895, 0.939, 0.928, 0.918, 0.93, 0.913, 0.92, 0.922, 0.921, 0.914, 0.911, 0.913, 0.899, 0.89, 0.892, 0.89, 0.882, 0.927, 0.925, 0.914, 0.918, 0.914, 0.911, 0.929, 0.927, 0.907, 0.919, 0.917, 0.92, 0.923, 0.906, 0.914, 0.928, 0.919, 0.898, 0.911, 0.902, 0.901, 0.907, 0.909, 0.923, 0.923, 0.909, 0.913, 0.928, 0.917, 0.917, 0.916, 0.915]\n","test_accuracy_w1=[0.729, 0.56, 0.663, 0.764, 0.714, 0.778, 0.788, 0.804, 0.765, 0.79, 0.849, 0.815, 0.811, 0.672, 0.781, 0.83, 0.854, 0.786, 0.798, 0.887, 0.84, 0.779, 0.789, 0.868, 0.837, 0.822, 0.841, 0.824, 0.867, 0.809, 0.817, 0.738, 0.844, 0.802, 0.845, 0.836, 0.848, 0.817, 0.825, 0.805, 0.816, 0.86, 0.829, 0.819, 0.873, 0.838, 0.818, 0.862, 0.813, 0.858, 0.803, 0.823, 0.84, 0.857, 0.849, 0.852, 0.85, 0.864, 0.863, 0.851, 0.859, 0.835, 0.864, 0.875, 0.846, 0.82, 0.849, 0.86, 0.832, 0.864, 0.89, 0.849, 0.858, 0.878, 0.852, 0.793, 0.878, 0.831, 0.86, 0.873, 0.845, 0.844, 0.881, 0.855, 0.868, 0.824, 0.864, 0.85, 0.874, 0.876, 0.877, 0.873, 0.855, 0.868, 0.858, 0.838, 0.877, 0.866, 0.842, 0.846, 0.83, 0.815, 0.83, 0.9, 0.851, 0.853, 0.827, 0.837, 0.858, 0.857, 0.847, 0.816, 0.866, 0.861, 0.839, 0.85, 0.821, 0.851, 0.847, 0.842, 0.847, 0.874, 0.829, 0.883, 0.871, 0.879, 0.897, 0.858, 0.842, 0.845, 0.859, 0.858, 0.837, 0.863, 0.891, 0.863, 0.886, 0.841, 0.846, 0.892, 0.866, 0.888, 0.874, 0.883, 0.849, 0.856, 0.841, 0.841, 0.859, 0.855, 0.859, 0.881, 0.846, 0.883, 0.862, 0.873, 0.876, 0.854, 0.863, 0.863, 0.882, 0.862, 0.877, 0.88, 0.89, 0.886, 0.858, 0.883, 0.843, 0.869, 0.857, 0.86, 0.861, 0.857, 0.841, 0.882, 0.844, 0.879, 0.86, 0.878, 0.875, 0.865, 0.822, 0.843, 0.888, 0.877, 0.84, 0.882, 0.846, 0.865, 0.882, 0.886, 0.878, 0.86, 0.865, 0.834, 0.863, 0.875, 0.863, 0.846, 0.868, 0.862, 0.846, 0.843, 0.858, 0.85, 0.87, 0.867, 0.881, 0.852, 0.874, 0.85, 0.865, 0.885, 0.853, 0.871, 0.855, 0.848, 0.873, 0.844, 0.846, 0.843, 0.854, 0.858, 0.877, 0.875, 0.856, 0.853, 0.866, 0.887, 0.894, 0.825, 0.875, 0.882, 0.825, 0.879, 0.886, 0.873, 0.852, 0.844, 0.886, 0.843, 0.908, 0.852, 0.832, 0.849, 0.876, 0.851, 0.888, 0.885, 0.808, 0.893, 0.836, 0.826, 0.872, 0.891, 0.861, 0.892, 0.866, 0.853, 0.866, 0.856, 0.871, 0.858, 0.895, 0.881, 0.854, 0.853, 0.879, 0.882, 0.882, 0.871, 0.867, 0.883, 0.877, 0.867, 0.848, 0.868, 0.879, 0.858, 0.882, 0.827, 0.869, 0.879, 0.879, 0.878, 0.832, 0.89, 0.845, 0.858, 0.826, 0.873, 0.868, 0.823, 0.845, 0.873, 0.886, 0.881, 0.865, 0.867]\n","train_accuracy_w2 = \n","test_accuracy_w2 = \n","train_accuracy_normal =\n","test_accuracy_normal = \n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YcM-SjbLqiVH","colab_type":"code","colab":{}},"cell_type":"code","source":["####get train_accuracy_w1 and normal from text file\n","#plot \n","plt.figure(figsize=(12, 6))\n","plt.title('EMNIST-letters test data using transferred initial weights vs. normal distribution (zoomed)')\n","plt.plot(epoch,test_accuracy_normal, label='normal dist')\n","plt.plot(epoch,test_accuracy_usingw1, label='transferred weights')\n","plt.xlabel('epoch')\n","plt.ylabel('accuracy')\n","plt.legend()\n","\n"],"execution_count":0,"outputs":[]}]}